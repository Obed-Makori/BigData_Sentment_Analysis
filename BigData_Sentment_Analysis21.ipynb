{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "BigData_Sentment_Analysis.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t1k8LOVuwLH"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSdvssZlmuOk"
      },
      "source": [
        "import io\n",
        "import random\n",
        "import string\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import words\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.sentiment.util import *\n",
        "\n",
        "\n",
        "# sklearn imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "# python imports\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from collections import Counter\n",
        "import datetime as dt\n",
        "\n",
        "\n",
        "# Visualization\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import ticker\n",
        "import seaborn as sns\n",
        "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
        "from wordcloud import WordCloud\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "\n",
        "# Saving models\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6SL4MoeuwLL"
      },
      "source": [
        "### Reading Tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZTEz8SAmxiT"
      },
      "source": [
        "# set an environment for Kaggle API\n",
        "os.environ['KAGGLE_CONFIG_DIR']='/content'\n",
        "\n",
        "!kaggle datasets download -d smid80/coronavirus-covid19-tweets-early-april\n",
        "#un zipping the file\n",
        "!unzip \\*.zip \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# reading all the data of different dates separately\n",
        "df1=pd.read_csv('2020-03-29 Coronavirus Tweets.CSV')\n",
        "df2=pd.read_csv('2020-03-30 Coronavirus Tweets.CSV')\n",
        "df3=pd.read_csv('2020-03-31 Coronavirus Tweets.CSV')\n",
        "df4=pd.read_csv('2020-04-01 Coronavirus Tweets.CSV')\n",
        "df5=pd.read_csv('2020-04-02 Coronavirus Tweets.CSV')\n",
        "df6=pd.read_csv('2020-04-03 Coronavirus Tweets.CSV')\n",
        "df7=pd.read_csv('2020-04-04 Coronavirus Tweets.CSV')\n",
        "df8=pd.read_csv('2020-04-05 Coronavirus Tweets.CSV')\n",
        "df10=pd.read_csv('2020-04-06 Coronavirus Tweets.CSV')\n",
        "df11=pd.read_csv('2020-04-07 Coronavirus Tweets.CSV')\n",
        "df12=pd.read_csv('2020-04-08 Coronavirus Tweets.CSV')\n",
        "df13=pd.read_csv('2020-04-09 Coronavirus Tweets.CSV')\n",
        "df14=pd.read_csv('2020-04-10 Coronavirus Tweets.CSV')\n",
        "df15=pd.read_csv('2020-04-11 Coronavirus Tweets.CSV')\n",
        "df16=pd.read_csv('2020-04-12 Coronavirus Tweets.CSV')\n",
        "df17=pd.read_csv('2020-04-13 Coronavirus Tweets.CSV')\n",
        "df18=pd.read_csv('2020-04-14 Coronavirus Tweets.CSV')\n",
        "df9=pd.read_csv('2020-04-15 Coronavirus Tweets.CSV')\n",
        "#concatenating all the dataset\n",
        "df=pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18],axis=0)\n",
        "\n",
        "#printing the fist five records\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0enJKF45mbbM"
      },
      "source": [
        "# droppping columns with missing values\n",
        "df.dropna(axis=1, inplace=True)a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4PQNum_uwLN"
      },
      "source": [
        "### Temporal frequency of tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7V7H1OKhuwLO"
      },
      "source": [
        "fig = plt.figure(figsize=(20,8))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set(title='Temporal tweet frequency worldwide', xlabel='Time', ylabel='Tweet frequency per hour')\n",
        "plt.hist(pd.to_datetime(df.created_at), bins = 24*9, color = 'b')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtmfKJJLuwLO"
      },
      "source": [
        "### Picking out the tweet texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "keqYt6vWuwLP"
      },
      "source": [
        "text_en = df['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWzSmEzluwLP"
      },
      "source": [
        "### Removing URLs from tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oLJzPQlXuwLP"
      },
      "source": [
        "text_en_lr = text_en.apply(lambda x: re.sub(r\"https\\S+\", \"\", str(x)))\n",
        "text_en_lr.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1XLX7jLuwLQ"
      },
      "source": [
        "### Converting all tweets to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PuqpZQNcuwLQ"
      },
      "source": [
        "text_en_lr_lc = text_en_lr.apply(lambda x: x.lower())\n",
        "text_en_lr_lc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxvz6vpBuwLR"
      },
      "source": [
        "### Removing punctuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6Ahr-R5YuwLR"
      },
      "source": [
        "text_en_lr_lc_pr = text_en_lr_lc.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
        "text_en_lr_lc_pr.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuPWuE7RuwLS"
      },
      "source": [
        "### Removing stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Hnz2pxaRuwLS"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['#coronavirus', '#coronavirusoutbreak', '#coronavirusPandemic', '#covid19', '#covid_19', '#epitwitter', '#ihavecorona', 'amp', 'coronavirus', 'covid19'])\n",
        "\n",
        "text_en_lr_lc_pr_sr = text_en_lr_lc_pr.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "text_en_lr_lc_pr_sr.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAoXFg0KuwLS"
      },
      "source": [
        "### Concatenating all the tweets into a list of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kEhvFTFHuwLT"
      },
      "source": [
        "word_list = [word for line in text_en_lr_lc_pr_sr for word in line.split()]\n",
        "word_list[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckgONRLXuwLT"
      },
      "source": [
        "### Calculating the Term Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "04bsfp4puwLT"
      },
      "source": [
        "sns.set(style=\"darkgrid\")\n",
        "counts = Counter(word_list).most_common(50)\n",
        "counts_df = pd.DataFrame(counts)\n",
        "counts_df\n",
        "counts_df.columns = ['word', 'frequency']\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12, 12))\n",
        "ax = sns.barplot(y=\"word\", x='frequency', ax = ax, data=counts_df)\n",
        "plt.savefig('wordcount_bar.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AwJVfjDPuwLU"
      },
      "source": [
        "wordcloud = WordCloud(\n",
        "    background_color='black',\n",
        "    max_words=50,\n",
        "    max_font_size=40, \n",
        "    scale=5,\n",
        "    random_state=1,\n",
        "    collocations=False,\n",
        "    normalize_plurals=False\n",
        ").generate(' '.join(word_list))\n",
        "\n",
        "\n",
        "plt.figure(figsize = (12, 10), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.savefig('wordcloud.png')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO8OE42quwLU"
      },
      "source": [
        "## Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYB2EN85uwLU"
      },
      "source": [
        "### Getting the polarity scores for each tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wMlFqKbEuwLU"
      },
      "source": [
        "sid = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = text_en_lr_lc_pr_sr.apply(lambda x: sid.polarity_scores(x))\n",
        "sent_scores_df = pd.DataFrame(list(sentiment_scores))\n",
        "sent_scores_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9HB0VIBuwLV"
      },
      "source": [
        "### Classifying the scores based on the compount polarity value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "x_UpCTFpuwLV"
      },
      "source": [
        "sent_scores_df['val'] = sent_scores_df['compound'].apply(lambda x: 'neutral' if x == 0 else ('positive' if x > 0 else 'negative'))\n",
        "sent_scores_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLjSbyQQuwLW"
      },
      "source": [
        "### Plotting the sentiment score counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aIYt0iRluwLW"
      },
      "source": [
        "sent_counts = pd.DataFrame.from_dict(Counter(sent_scores_df['val']), orient = 'index').reset_index()\n",
        "sent_counts.columns = ['sentiment', 'count']\n",
        "\n",
        "sns.barplot(y=\"count\", x='sentiment', data=sent_counts)\n",
        "plt.savefig('sentiment.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z14G2En1uwLW"
      },
      "source": [
        "### Temporal plot of the sentiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eLzgQ-aAuwLW"
      },
      "source": [
        "sentiments_time_df = pd.DataFrame()\n",
        "sentiments_time_df['time'] = df['created_at']\n",
        "sentiments_time_df['polarity'] = sent_scores_df['compound']\n",
        "sentiments_time_df.index = pd.to_datetime(sentiments_time_df['time'])\n",
        "\n",
        "\n",
        "ot = sentiments_time_df.sample(frac=.001)\n",
        "ot['time'] = pd.to_datetime(ot['time'])\n",
        "ot.index = pd.to_datetime(ot['time'])\n",
        "ot.sort_index(inplace=True)\n",
        "ot['expanding'] = ot['polarity'].expanding().mean()\n",
        "ot['rolling'] = ot['polarity'].rolling('1h').mean()\n",
        "\n",
        "fig = plt.figure(figsize=(20,5))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.scatter(ot['time'],ot['polarity'], label='Tweet Sentiment', s = 10, color = 'y')\n",
        "ax.plot(ot['time'],ot['rolling'], color ='r', label='Rolling Mean', linewidth = 5)\n",
        "ax.plot(ot['time'],ot['expanding'], color='b', label='Expanding Mean', linewidth = 5)\n",
        "ax.set_xlim([dt.date(2020,4,3),dt.date(2020,4,3)])\n",
        "ax.set(title='Tweet Sentiments over Time', xlabel='Date', ylabel='Sentiment polarity')\n",
        "ax.legend(loc='best')\n",
        "fig.tight_layout()\n",
        "plt.savefig('temporal_sentiments.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-eWolFmuwLX"
      },
      "source": [
        "### Sentiment scores distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "--JHnvYXuwLX"
      },
      "source": [
        "fig = plt.figure(figsize=(10,5))\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set(title='Tweet Sentiments distribution', xlabel='polarity', ylabel='frequency')\n",
        "sns.distplot(sentiments_time_df['polarity'], bins=30, ax=ax)\n",
        "# plt.show()\n",
        "plt.savefig('sentiment_distribution.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu9aZ1CruwLX"
      },
      "source": [
        "### Word cloud of polar words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "u6wgAgQJuwLY"
      },
      "source": [
        "polar_tweets_df = pd.DataFrame()\n",
        "polar_tweets_df['tweet'] = text_en_lr_lc_pr_sr\n",
        "polar_tweets_df['polarity'] = sent_scores_df['val']\n",
        "\n",
        "positive = polar_tweets_df[polar_tweets_df['polarity'] == 'positive']['tweet']\n",
        "negative = polar_tweets_df[polar_tweets_df['polarity'] == 'negative']['tweet']\n",
        "neutral = polar_tweets_df[polar_tweets_df['polarity'] == 'neutral']['tweet']\n",
        "\n",
        "positive_list = [word for line in positive for word in line.split()]\n",
        "negative_list = [word for line in negative for word in line.split()]\n",
        "neutral_list = [word for line in neutral for word in line.split()]\n",
        "\n",
        "positive_cloud = WordCloud(\n",
        "    background_color='black',\n",
        "    max_words=50,\n",
        "    max_font_size=40, \n",
        "    scale=5,\n",
        "    random_state=1,\n",
        "    collocations=False,\n",
        "    normalize_plurals=False\n",
        ").generate(' '.join(positive_list))\n",
        "\n",
        "negative_cloud = WordCloud(\n",
        "    background_color='black',\n",
        "    max_words=50,\n",
        "    max_font_size=40, \n",
        "    scale=5,\n",
        "    random_state=1,\n",
        "    collocations=False,\n",
        "    normalize_plurals=False\n",
        ").generate(' '.join(negative_list))\n",
        "\n",
        "neutral_cloud = WordCloud(\n",
        "    background_color='black',\n",
        "    max_words=50,\n",
        "    max_font_size=40, \n",
        "    scale=5,\n",
        "    random_state=1,\n",
        "    collocations=False,\n",
        "    normalize_plurals=False\n",
        ").generate(' '.join(neutral_list))\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize = (20, 12))\n",
        "# fig.suptitle('Clouds of polar words', fontsize = 30)\n",
        "fig.tight_layout(pad = 0)\n",
        "\n",
        "axs[0, 0].imshow(positive_cloud)\n",
        "axs[0, 0].set_title('Words from positive tweets', fontsize = 20)\n",
        "axs[0, 0].axis('off')\n",
        "# axs[0, 0].tight_layout(pad = 1)\n",
        "\n",
        "axs[0, 1].imshow(negative_cloud)\n",
        "axs[0, 1].set_title('Words from negative tweets', fontsize = 20)\n",
        "axs[0, 1].axis('off')\n",
        "# axs[0, 1].tight_layout(pad = 1)\n",
        "\n",
        "axs[1, 0].imshow(neutral_cloud)\n",
        "axs[1, 0].set_title('Words from neutral tweets', fontsize = 20)\n",
        "axs[1, 0].axis('off')\n",
        "# axs[1, 0].tight_layout(pad = 1)\n",
        "\n",
        "axs[1, 1].imshow(wordcloud)\n",
        "axs[1, 1].set_title('Words from all tweets', fontsize = 20)\n",
        "axs[1, 1].axis('off')\n",
        "# axs[1, 0].tight_layout(pad = 1)\n",
        "plt.savefig('joint_cloud.png')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}